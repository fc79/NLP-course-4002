{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452910c2",
   "metadata": {},
   "source": [
    "# بخش اول"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91d2f2b",
   "metadata": {},
   "source": [
    "برای انجام بخش اول پروژه و با توجه به هدف پروژه، از ابزار SentenceTransformers استفاده می شود. این ابزار به بیان ساده از یک مدل BERT و یک لابه Pooling تشکیل شده است. مدل BERT بردار های کلمات را به وجود می آورد و لایه Pooling میانگین این بردار ها را به عنوان خروجی مدل برای جمله داده شده، می سازد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d609573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util, InputExample, evaluation, losses\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "from functools import partial\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb7f5e",
   "metadata": {},
   "source": [
    "با توجه به اینکه داده ها به زبان انگلیسی هستند، از مدل bert-base-uncased استفاده شده است."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ee7e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "transformer = SentenceTransformer(model_name)\n",
    "print(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea87780",
   "metadata": {},
   "source": [
    "همانطور که مشاهده می شود، این مدل از یک مدل BERT و یک لایه Poolong تشکیل شده است."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c4d3ca",
   "metadata": {},
   "source": [
    "برای شروع داده های پیش پردازش شده را وارد می کنیم و با توجه به اینکه ورودی مدل باید جملات کامل باشند، با استفاده از تابع `make_string` جملات پردازش شده را می سازیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc70f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/no_stopwords/quora-question-pairs.pk', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b96e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[:, ['question1', 'question2', 'question1_no_stopwords', 'question2_no_stopwords', 'is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b7122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_string(words_list):\n",
    "    return \" \".join(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['question1_processed'] = data['question1_no_stopwords'].apply(make_string)\n",
    "data['question2_processed'] = data['question2_no_stopwords'].apply(make_string)\n",
    "\n",
    "del data['question1_no_stopwords'], data['question2_no_stopwords'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e48c62",
   "metadata": {},
   "source": [
    "برای استفاده از این مدل، ابتدا همه ی جملات موجود در دیتاست را encode می کنیم. سپس با استفاده از معیار cosine_similarity میزان شباهت بردار های متناظر جملات را پیدا می کنیم. در انتها باید یک عدد مشخص به عنوان حد مرجع پیدا کنیم تا بتوانیم جملاتی که میزان شباهت آن ها از حد مرجع بیشتر بود را برابر و جملاتی که میزان شباهت بردار های آنها کمتر از حد مرجع بود را نابرابر تشخیص دهیم (در واقع خد مرجع یک hyper-parameter از مدل است که با استفاده از داده تست مقدار بهینه آن معین می شود)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace1169",
   "metadata": {},
   "source": [
    "با توحه به اینکه مدل BERT برای ساختن بردار کلمات به کلمات مجاور نیز حساس است، ممکن است حذف کردن کلمات اضافی و پیش پردازش های این چنینی باعث مختل شدن عملکرد مدل شود. برای بررسی این مسئله، علاوه بر استفاده از داده های پیش پردازش شده، داده های خام و دست نخورده نیز استفاده می شوند. به همین منظور، بردار های جملات هم برای جملات پیش پردازش شده و هم برای جملات خام محاسبه می شود."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2124e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_questions = list(set(data['question1'].to_list() + data['question2'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510faba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_questions = list(set(data['question1_processed'].to_list() + data['question2_processed'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e620f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_questions), len(processed_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586013bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_questions_embeddings = transformer.encode(raw_questions,\n",
    "                                              show_progress_bar=True,\n",
    "                                              device='cuda')\n",
    "raw_questions_embeddings_dict = {key: value for key, value in zip(raw_questions, raw_questions_embeddings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41422dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_questions_embeddings = transformer.encode(processed_questions,\n",
    "                                                    show_progress_bar=True,\n",
    "                                                    device='cuda')\n",
    "processed_questions_embeddings_dict = {key: value for key, value in zip(processed_questions, processed_questions_embeddings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32825504",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_questions_embeddings_dict), len(processed_questions_embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['question1_embeddings'] = [raw_questions_embeddings_dict.get(text) for text in data['question1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['question2_embeddings'] = [raw_questions_embeddings_dict.get(text) for text in data['question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8194e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['question2_processed_embeddings'] = [processed_questions_embeddings_dict.get(text) for text in data['question2_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['question1_processed_embeddings'] = [processed_questions_embeddings_dict.get(text) for text in data['question1_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363be8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_similarities, processed_similarities = [], []\n",
    "for item in tqdm.tqdm(data.itertuples(), total=len(data)):\n",
    "    raw_similarities.append(float(util.cos_sim(item.question1_embeddings,\n",
    "                                         item.question2_embeddings)))\n",
    "    processed_similarities.append(float(util.cos_sim(item.question1_processed_embeddings,\n",
    "                                               item.question2_processed_embeddings)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b1b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['raw_similarities'] = raw_similarities\n",
    "data['processed_similarities'] = processed_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2894191",
   "metadata": {},
   "source": [
    "پس از ساخت داده ها، آن را برای استفاده های بعدی ذخیره می کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data-with-cosine.pk', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589a4f7",
   "metadata": {},
   "source": [
    "برای ادامه، از داده های آماده شده استفاده می کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c546eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data-with-cosine.pk', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f81762c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_processed</th>\n",
       "      <th>question2_processed</th>\n",
       "      <th>question1_embeddings</th>\n",
       "      <th>question2_embeddings</th>\n",
       "      <th>question2_processed_embeddings</th>\n",
       "      <th>question1_processed_embeddings</th>\n",
       "      <th>raw_similarities</th>\n",
       "      <th>processed_similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "      <td>[0.07440116, -0.32690492, -0.05289258, -0.0950...</td>\n",
       "      <td>[0.16965555, -0.11540019, 0.025943296, -0.0977...</td>\n",
       "      <td>[0.22033955, -0.23850112, 0.21932186, -0.31143...</td>\n",
       "      <td>[0.06516282, -0.22289737, 0.24185374, -0.19887...</td>\n",
       "      <td>0.959752</td>\n",
       "      <td>0.960769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>story kohinoor kohinoor diamond</td>\n",
       "      <td>would happen indian government stole kohinoor ...</td>\n",
       "      <td>[-0.018936362, -0.08034966, -0.18317325, -0.38...</td>\n",
       "      <td>[0.20551735, -0.19547899, -0.20875359, -0.2068...</td>\n",
       "      <td>[-0.10619234, -0.08733711, 0.11114665, -0.0623...</td>\n",
       "      <td>[-0.00389089, 0.07283777, -0.0031249835, -0.14...</td>\n",
       "      <td>0.879686</td>\n",
       "      <td>0.784556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>increase speed internet connection using vpn</td>\n",
       "      <td>internet speed increased hacking dns</td>\n",
       "      <td>[0.123759374, -0.002891716, 0.1179015, 0.09822...</td>\n",
       "      <td>[0.32502264, -0.039571997, -0.17964831, 0.3245...</td>\n",
       "      <td>[0.13909794, -0.27083716, -0.21249552, 0.33494...</td>\n",
       "      <td>[0.052815385, -0.38950598, 0.061434872, 0.3603...</td>\n",
       "      <td>0.882472</td>\n",
       "      <td>0.842644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>mentally lonely solve</td>\n",
       "      <td>find remainder mathmath divided</td>\n",
       "      <td>[-0.31565902, 0.45589238, 0.09193595, -0.29455...</td>\n",
       "      <td>[0.109701306, -0.17969146, 0.45731202, -0.3672...</td>\n",
       "      <td>[-0.090969235, -0.2886003, 0.074688174, -0.087...</td>\n",
       "      <td>[-0.18023984, 0.18657511, -0.13466242, 0.00032...</td>\n",
       "      <td>0.537907</td>\n",
       "      <td>0.629015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>one dissolve water quikly sugar salt methane c...</td>\n",
       "      <td>fish would survive salt water</td>\n",
       "      <td>[-0.07246317, 0.5680828, 0.14501809, 0.1467610...</td>\n",
       "      <td>[0.09010597, -0.12469902, -0.34981173, 0.23235...</td>\n",
       "      <td>[0.12829192, 0.13424908, -0.39034936, 0.321467...</td>\n",
       "      <td>[0.032609444, 0.40425083, 0.19903037, 0.117206...</td>\n",
       "      <td>0.674249</td>\n",
       "      <td>0.548204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "      <td>astrology capricorn sun cap moon cap risingwha...</td>\n",
       "      <td>im triple capricorn sun moon ascendant caprico...</td>\n",
       "      <td>[0.26019007, 0.40109852, 0.3073888, -0.2745735...</td>\n",
       "      <td>[0.17183104, 0.31335947, 0.42230237, -0.293117...</td>\n",
       "      <td>[-0.213034, 0.13180369, 0.37939465, -0.2649056...</td>\n",
       "      <td>[0.12810151, 0.47621626, 0.36973673, -0.281290...</td>\n",
       "      <td>0.874022</td>\n",
       "      <td>0.833304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>buy tiago</td>\n",
       "      <td>keeps childern active far phone video games</td>\n",
       "      <td>[0.16166113, -0.2472522, -0.035803825, -0.0062...</td>\n",
       "      <td>[0.22253351, 0.06014577, 0.14697327, -0.175406...</td>\n",
       "      <td>[-0.012636554, 0.077844724, 0.12232236, -0.196...</td>\n",
       "      <td>[0.11120359, -0.32616764, -0.075483814, 0.0835...</td>\n",
       "      <td>0.587677</td>\n",
       "      <td>0.528094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "      <td>good geologist</td>\n",
       "      <td>great geologist</td>\n",
       "      <td>[0.06402613, 0.29727694, -0.17376241, -0.18346...</td>\n",
       "      <td>[0.1902708, 0.34478855, -0.27575397, -0.253990...</td>\n",
       "      <td>[-0.1553924, 0.40152916, -0.110961795, 0.19542...</td>\n",
       "      <td>[-0.002600506, 0.33528686, -0.009903578, 0.267...</td>\n",
       "      <td>0.880762</td>\n",
       "      <td>0.849039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>use instead</td>\n",
       "      <td>use instead</td>\n",
       "      <td>[0.0016314604, -0.2889597, -0.30436084, -0.360...</td>\n",
       "      <td>[0.1314659, -0.06270888, 0.03523148, -0.251690...</td>\n",
       "      <td>[0.16303095, -0.0365623, -0.07809156, -0.05447...</td>\n",
       "      <td>[0.16303095, -0.0365623, -0.07809156, -0.05447...</td>\n",
       "      <td>0.771418</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "      <td>motorola company hack charter motorolla dcx</td>\n",
       "      <td>hack motorola dcx free internet</td>\n",
       "      <td>[0.24876459, -0.173902, 0.36824298, 0.34958592...</td>\n",
       "      <td>[0.34119752, -0.0065160315, 0.21360372, 0.2436...</td>\n",
       "      <td>[0.20284963, 0.08944458, 0.087624736, 0.397783...</td>\n",
       "      <td>[0.35393187, -0.1942975, 0.20227328, 0.4051149...</td>\n",
       "      <td>0.828769</td>\n",
       "      <td>0.801506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "8                    When do you use シ instead of し?   \n",
       "9  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...             1   \n",
       "6  What keeps childern active and far from phone ...             0   \n",
       "7          What should I do to be a great geologist?             1   \n",
       "8              When do you use \"&\" instead of \"and\"?             0   \n",
       "9  How do I hack Motorola DCX3400 for free internet?             0   \n",
       "\n",
       "                                 question1_processed  \\\n",
       "0          step step guide invest share market india   \n",
       "1                    story kohinoor kohinoor diamond   \n",
       "2       increase speed internet connection using vpn   \n",
       "3                              mentally lonely solve   \n",
       "4  one dissolve water quikly sugar salt methane c...   \n",
       "5  astrology capricorn sun cap moon cap risingwha...   \n",
       "6                                          buy tiago   \n",
       "7                                     good geologist   \n",
       "8                                        use instead   \n",
       "9        motorola company hack charter motorolla dcx   \n",
       "\n",
       "                                 question2_processed  \\\n",
       "0                step step guide invest share market   \n",
       "1  would happen indian government stole kohinoor ...   \n",
       "2               internet speed increased hacking dns   \n",
       "3                    find remainder mathmath divided   \n",
       "4                      fish would survive salt water   \n",
       "5  im triple capricorn sun moon ascendant caprico...   \n",
       "6        keeps childern active far phone video games   \n",
       "7                                    great geologist   \n",
       "8                                        use instead   \n",
       "9                    hack motorola dcx free internet   \n",
       "\n",
       "                                question1_embeddings  \\\n",
       "0  [0.07440116, -0.32690492, -0.05289258, -0.0950...   \n",
       "1  [-0.018936362, -0.08034966, -0.18317325, -0.38...   \n",
       "2  [0.123759374, -0.002891716, 0.1179015, 0.09822...   \n",
       "3  [-0.31565902, 0.45589238, 0.09193595, -0.29455...   \n",
       "4  [-0.07246317, 0.5680828, 0.14501809, 0.1467610...   \n",
       "5  [0.26019007, 0.40109852, 0.3073888, -0.2745735...   \n",
       "6  [0.16166113, -0.2472522, -0.035803825, -0.0062...   \n",
       "7  [0.06402613, 0.29727694, -0.17376241, -0.18346...   \n",
       "8  [0.0016314604, -0.2889597, -0.30436084, -0.360...   \n",
       "9  [0.24876459, -0.173902, 0.36824298, 0.34958592...   \n",
       "\n",
       "                                question2_embeddings  \\\n",
       "0  [0.16965555, -0.11540019, 0.025943296, -0.0977...   \n",
       "1  [0.20551735, -0.19547899, -0.20875359, -0.2068...   \n",
       "2  [0.32502264, -0.039571997, -0.17964831, 0.3245...   \n",
       "3  [0.109701306, -0.17969146, 0.45731202, -0.3672...   \n",
       "4  [0.09010597, -0.12469902, -0.34981173, 0.23235...   \n",
       "5  [0.17183104, 0.31335947, 0.42230237, -0.293117...   \n",
       "6  [0.22253351, 0.06014577, 0.14697327, -0.175406...   \n",
       "7  [0.1902708, 0.34478855, -0.27575397, -0.253990...   \n",
       "8  [0.1314659, -0.06270888, 0.03523148, -0.251690...   \n",
       "9  [0.34119752, -0.0065160315, 0.21360372, 0.2436...   \n",
       "\n",
       "                      question2_processed_embeddings  \\\n",
       "0  [0.22033955, -0.23850112, 0.21932186, -0.31143...   \n",
       "1  [-0.10619234, -0.08733711, 0.11114665, -0.0623...   \n",
       "2  [0.13909794, -0.27083716, -0.21249552, 0.33494...   \n",
       "3  [-0.090969235, -0.2886003, 0.074688174, -0.087...   \n",
       "4  [0.12829192, 0.13424908, -0.39034936, 0.321467...   \n",
       "5  [-0.213034, 0.13180369, 0.37939465, -0.2649056...   \n",
       "6  [-0.012636554, 0.077844724, 0.12232236, -0.196...   \n",
       "7  [-0.1553924, 0.40152916, -0.110961795, 0.19542...   \n",
       "8  [0.16303095, -0.0365623, -0.07809156, -0.05447...   \n",
       "9  [0.20284963, 0.08944458, 0.087624736, 0.397783...   \n",
       "\n",
       "                      question1_processed_embeddings  raw_similarities  \\\n",
       "0  [0.06516282, -0.22289737, 0.24185374, -0.19887...          0.959752   \n",
       "1  [-0.00389089, 0.07283777, -0.0031249835, -0.14...          0.879686   \n",
       "2  [0.052815385, -0.38950598, 0.061434872, 0.3603...          0.882472   \n",
       "3  [-0.18023984, 0.18657511, -0.13466242, 0.00032...          0.537907   \n",
       "4  [0.032609444, 0.40425083, 0.19903037, 0.117206...          0.674249   \n",
       "5  [0.12810151, 0.47621626, 0.36973673, -0.281290...          0.874022   \n",
       "6  [0.11120359, -0.32616764, -0.075483814, 0.0835...          0.587677   \n",
       "7  [-0.002600506, 0.33528686, -0.009903578, 0.267...          0.880762   \n",
       "8  [0.16303095, -0.0365623, -0.07809156, -0.05447...          0.771418   \n",
       "9  [0.35393187, -0.1942975, 0.20227328, 0.4051149...          0.828769   \n",
       "\n",
       "   processed_similarities  \n",
       "0                0.960769  \n",
       "1                0.784556  \n",
       "2                0.842644  \n",
       "3                0.629015  \n",
       "4                0.548204  \n",
       "5                0.833304  \n",
       "6                0.528094  \n",
       "7                0.849039  \n",
       "8                1.000000  \n",
       "9                0.801506  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3069bb5b",
   "metadata": {},
   "source": [
    "برای پیدا کردن بهترین دقت، بازه ای از حد مرجع ها را در نظر می گیریم و برای هر عدد از این بازه، دقت مدل در پیدا کردن جملات مشابه را حساب می کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db830092",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.3, 1.0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f58b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_accuracy(similarity_scores, labels, threshold):\n",
    "    preds = []\n",
    "    for i in similarity_scores:\n",
    "        prediction = int(i >= threshold)\n",
    "        preds.append(prediction)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return round(accuracy * 100, 2)\n",
    "\n",
    "def get_accuracies(scores, labels, thresholds):\n",
    "    accuracies = []\n",
    "    for threshold in tqdm.tqdm(thresholds):\n",
    "        accuracy = similarity_accuracy(scores, labels, threshold)\n",
    "        accuracies.append((threshold, accuracy))\n",
    "    return sorted(accuracies, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8df5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:21<00:00,  3.31it/s]\n",
      "100%|██████████| 70/70 [00:21<00:00,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8800000000000006, 68.75),\n",
      " (0.8700000000000006, 68.6),\n",
      " (0.8900000000000006, 68.6),\n",
      " (0.8600000000000005, 68.17),\n",
      " (0.9000000000000006, 68.15),\n",
      " (0.9100000000000006, 67.5),\n",
      " (0.8500000000000005, 67.4),\n",
      " (0.9200000000000006, 66.77),\n",
      " (0.8400000000000005, 66.36),\n",
      " (0.9300000000000006, 66.0)]\n",
      "########################################\n",
      "[(0.8600000000000005, 66.16),\n",
      " (0.8500000000000005, 66.15),\n",
      " (0.8700000000000006, 66.06),\n",
      " (0.8400000000000005, 66.03),\n",
      " (0.8800000000000006, 65.94),\n",
      " (0.8300000000000005, 65.82),\n",
      " (0.8900000000000006, 65.72),\n",
      " (0.8200000000000005, 65.49),\n",
      " (0.9000000000000006, 65.48),\n",
      " (0.9100000000000006, 65.26)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_accuracies = get_accuracies(data['raw_similarities'], data['is_duplicate'], thresholds)\n",
    "processed_accuracies = get_accuracies(data['processed_similarities'], data['is_duplicate'], thresholds)\n",
    "\n",
    "pprint(raw_accuracies[:10])\n",
    "print('#' * 40)\n",
    "pprint(processed_accuracies[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09674c6e",
   "metadata": {},
   "source": [
    "نکته مهمی که از دقت های به دست آمده می توان برداشت کرد، این است که مدل ما روی داده های پیش پردازش نشده و خام می تواند به دقت های بالاتری در تشخیص عبارات مشابه برسد. برای بررسی بیشتر، برای fine tune کردن مدل، یکبار از داده های خام و یکبار از داده های پیش پردازش شده استفاده می کنیم و نتابج را با یکدیگر مقایسه می کنیم. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb8d81",
   "metadata": {},
   "source": [
    "برای شروع fine tune، ایندا داده های آموزش و تست را از یکدیگر و به نسبت گفته شده جدا می کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d31bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.8\n",
    "train_val_split = int(len(data) * train_percent)\n",
    "\n",
    "train_data = data.iloc[:train_val_split, :]\n",
    "validation_data = data.iloc[train_val_split:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9abbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66faae8",
   "metadata": {},
   "source": [
    "مدل SentenceTransformer برای آموزش و نیز تست به زوج سوالات و میزان شباهت آن ها به هم نیاز دارد. به همین منظور کلاس InputExample را برای استفاده از مدل ارائه داده است. با استفاده از این کلاس و نیز DataLoader ابزار pytorch داده های آموزش و ارزیابی (هم برای داده های پردازش شده و هم برای داده های خام) آماده می شوند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac386dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_examples = []\n",
    "for row in tqdm.tqdm(train_data.itertuples(), total=len(train_data)):\n",
    "    example = InputExample(texts=[row.question1, row.question2], label=float(row.is_duplicate))\n",
    "    raw_train_examples.append(example)\n",
    "raw_dataloader = DataLoader(raw_train_examples, shuffle=True, batch_size=32)\n",
    "\n",
    "raw_validation_examples = []\n",
    "for row in tqdm.tqdm(validation_data.itertuples(), total=len(validation_data)):\n",
    "    example = InputExample(texts=[row.question1, row.question2], label=float(row.is_duplicate))\n",
    "    raw_validation_examples.append(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_examples = []\n",
    "for row in tqdm.tqdm(train_data.itertuples(), total=len(train_data)):\n",
    "    example = InputExample(texts=[row.question1_processed, row.question2_processed], label=float(row.is_duplicate))\n",
    "    processed_train_examples.append(example)\n",
    "processed_dataloader = DataLoader(processed_train_examples, shuffle=True, batch_size=32)\n",
    "\n",
    "processed_validation_examples = []\n",
    "for row in tqdm.tqdm(validation_data.itertuples(), total=len(validation_data)):\n",
    "    example = InputExample(texts=[row.question1_processed, row.question2_processed], label=float(row.is_duplicate))\n",
    "    processed_validation_examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61c2ee",
   "metadata": {},
   "source": [
    "مدل های استفاده شده برای آموزش به همراه loss function مورد نیاز آماده می شوند. شیوه آموزش در ابزار SentenceTransformer برای تسک تشخیص شباهت به این شکل است که ابتدا دو شبکه مجزا اما با وزن های برابر ساخته می شوند (در واقع وزن های این دو شبکه از یک منبع می آیند و با تعییر وزن های یک شبکه، وزن های شبکه دیگر نیز تغییر می کنند)؛ در واقع یک شبکه Siamese داریم. دو جمله ورودی به صورت مجزا و مستقل از هم تبدیل به بردار می شوند (با استفاده از BERT و Pooling). سپس اندازه cosine siilarity دو بردار محاسبه می شود. اختلاف این شباهت و برچسب دو جمله (که آیا شبیه هستند یا خیر) به عنوان loss محاسبه شده و سپس وزن های شبکه آپدیت می شوند. در واقع در جریان یادگیری، تلاش داریم ویژگی های پدید آورنده اختلاف و شباهت بین دو جمله را به مدل های BERT یاد دهیم تا بتوانند بردار هایی تولید کنند که برای پیدا کردن شباهت دو جمله مناسب باشد.\n",
    "\n",
    "نتابج آموزش شبکه ها ذخیره می شود و در ادامه برای بررسی بیشتر بازبینی می شود."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac982976",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model = SentenceTransformer('bert-base-uncased')\n",
    "raw_evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(raw_validation_examples,\n",
    "                                                                            name='raw validation data',\n",
    "                                                                            show_progress_bar=True)\n",
    "raw_loss = losses.CosineSimilarityLoss(raw_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d52d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_model = SentenceTransformer('bert-base-uncased')\n",
    "processed_evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(processed_validation_examples,\n",
    "                                                                        name='processed validation data',\n",
    "                                                                        show_progress_bar=True)\n",
    "processed_loss = losses.CosineSimilarityLoss(processed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310019fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model.fit(train_objectives=[(raw_dataloader, raw_loss)],\n",
    "              epochs=5,\n",
    "              evaluator=raw_evaluator,\n",
    "              evaluation_steps=1000,\n",
    "              output_path=\"raw_model_training_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5cdae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_model.fit(train_objectives=[(processed_dataloader, processed_loss)],\n",
    "                    epochs=5,\n",
    "                    evaluator=processed_evaluator,\n",
    "                    evaluation_steps=1000,\n",
    "                    output_path=\"processed_model_training_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11238ca",
   "metadata": {},
   "source": [
    "پس از پایان آموزش، نتایج را باز می کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c82360a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model_train_results = pd.read_csv('raw_model_training_results/eval/similarity_evaluation_raw validation data_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33c72e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_model_train_results = pd.read_csv('processed_model_training_results/eval/similarity_evaluation_processed validation data_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aad9bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>steps</th>\n",
       "      <th>cosine_pearson</th>\n",
       "      <th>cosine_spearman</th>\n",
       "      <th>euclidean_pearson</th>\n",
       "      <th>euclidean_spearman</th>\n",
       "      <th>manhattan_pearson</th>\n",
       "      <th>manhattan_spearman</th>\n",
       "      <th>dot_pearson</th>\n",
       "      <th>dot_spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.468945</td>\n",
       "      <td>0.476004</td>\n",
       "      <td>0.403510</td>\n",
       "      <td>0.424756</td>\n",
       "      <td>0.403969</td>\n",
       "      <td>0.425364</td>\n",
       "      <td>0.487691</td>\n",
       "      <td>0.492376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.592885</td>\n",
       "      <td>0.607386</td>\n",
       "      <td>0.561215</td>\n",
       "      <td>0.572892</td>\n",
       "      <td>0.562487</td>\n",
       "      <td>0.574276</td>\n",
       "      <td>0.596222</td>\n",
       "      <td>0.598063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.639567</td>\n",
       "      <td>0.645881</td>\n",
       "      <td>0.618728</td>\n",
       "      <td>0.620769</td>\n",
       "      <td>0.619136</td>\n",
       "      <td>0.621277</td>\n",
       "      <td>0.638827</td>\n",
       "      <td>0.634351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.662008</td>\n",
       "      <td>0.663198</td>\n",
       "      <td>0.655926</td>\n",
       "      <td>0.650820</td>\n",
       "      <td>0.656257</td>\n",
       "      <td>0.651380</td>\n",
       "      <td>0.658362</td>\n",
       "      <td>0.650780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.680992</td>\n",
       "      <td>0.676275</td>\n",
       "      <td>0.677499</td>\n",
       "      <td>0.667278</td>\n",
       "      <td>0.677414</td>\n",
       "      <td>0.667357</td>\n",
       "      <td>0.675948</td>\n",
       "      <td>0.664827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.695814</td>\n",
       "      <td>0.688548</td>\n",
       "      <td>0.691513</td>\n",
       "      <td>0.679803</td>\n",
       "      <td>0.691554</td>\n",
       "      <td>0.680018</td>\n",
       "      <td>0.690855</td>\n",
       "      <td>0.678142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.704021</td>\n",
       "      <td>0.694879</td>\n",
       "      <td>0.702576</td>\n",
       "      <td>0.687894</td>\n",
       "      <td>0.702605</td>\n",
       "      <td>0.688137</td>\n",
       "      <td>0.699132</td>\n",
       "      <td>0.684231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>8000</td>\n",
       "      <td>0.712582</td>\n",
       "      <td>0.700064</td>\n",
       "      <td>0.708250</td>\n",
       "      <td>0.691851</td>\n",
       "      <td>0.708132</td>\n",
       "      <td>0.691822</td>\n",
       "      <td>0.707009</td>\n",
       "      <td>0.689909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>0.716562</td>\n",
       "      <td>0.702152</td>\n",
       "      <td>0.723412</td>\n",
       "      <td>0.700258</td>\n",
       "      <td>0.723382</td>\n",
       "      <td>0.700354</td>\n",
       "      <td>0.712686</td>\n",
       "      <td>0.693195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.724499</td>\n",
       "      <td>0.702371</td>\n",
       "      <td>0.720826</td>\n",
       "      <td>0.698476</td>\n",
       "      <td>0.720519</td>\n",
       "      <td>0.698312</td>\n",
       "      <td>0.719167</td>\n",
       "      <td>0.696440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.728147</td>\n",
       "      <td>0.708525</td>\n",
       "      <td>0.727829</td>\n",
       "      <td>0.704231</td>\n",
       "      <td>0.727771</td>\n",
       "      <td>0.704261</td>\n",
       "      <td>0.724349</td>\n",
       "      <td>0.701995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.733975</td>\n",
       "      <td>0.711244</td>\n",
       "      <td>0.737156</td>\n",
       "      <td>0.709016</td>\n",
       "      <td>0.737057</td>\n",
       "      <td>0.709074</td>\n",
       "      <td>0.730813</td>\n",
       "      <td>0.705273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.737847</td>\n",
       "      <td>0.719376</td>\n",
       "      <td>0.745958</td>\n",
       "      <td>0.716687</td>\n",
       "      <td>0.745550</td>\n",
       "      <td>0.716507</td>\n",
       "      <td>0.736410</td>\n",
       "      <td>0.712277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.746105</td>\n",
       "      <td>0.720037</td>\n",
       "      <td>0.748386</td>\n",
       "      <td>0.717082</td>\n",
       "      <td>0.748117</td>\n",
       "      <td>0.716998</td>\n",
       "      <td>0.744367</td>\n",
       "      <td>0.715522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.749436</td>\n",
       "      <td>0.719457</td>\n",
       "      <td>0.749008</td>\n",
       "      <td>0.716512</td>\n",
       "      <td>0.748800</td>\n",
       "      <td>0.716348</td>\n",
       "      <td>0.745617</td>\n",
       "      <td>0.714427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.755005</td>\n",
       "      <td>0.726465</td>\n",
       "      <td>0.757816</td>\n",
       "      <td>0.724752</td>\n",
       "      <td>0.757464</td>\n",
       "      <td>0.724589</td>\n",
       "      <td>0.752393</td>\n",
       "      <td>0.721438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.757062</td>\n",
       "      <td>0.725729</td>\n",
       "      <td>0.758183</td>\n",
       "      <td>0.723777</td>\n",
       "      <td>0.757483</td>\n",
       "      <td>0.723227</td>\n",
       "      <td>0.754772</td>\n",
       "      <td>0.721516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.762997</td>\n",
       "      <td>0.731823</td>\n",
       "      <td>0.764398</td>\n",
       "      <td>0.728876</td>\n",
       "      <td>0.764066</td>\n",
       "      <td>0.728718</td>\n",
       "      <td>0.761528</td>\n",
       "      <td>0.727469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>8000</td>\n",
       "      <td>0.767116</td>\n",
       "      <td>0.734692</td>\n",
       "      <td>0.767075</td>\n",
       "      <td>0.731277</td>\n",
       "      <td>0.766908</td>\n",
       "      <td>0.731326</td>\n",
       "      <td>0.765289</td>\n",
       "      <td>0.730022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>9000</td>\n",
       "      <td>0.767641</td>\n",
       "      <td>0.734926</td>\n",
       "      <td>0.771441</td>\n",
       "      <td>0.733516</td>\n",
       "      <td>0.771302</td>\n",
       "      <td>0.733475</td>\n",
       "      <td>0.765339</td>\n",
       "      <td>0.729942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.771579</td>\n",
       "      <td>0.734489</td>\n",
       "      <td>0.772224</td>\n",
       "      <td>0.732795</td>\n",
       "      <td>0.771878</td>\n",
       "      <td>0.732576</td>\n",
       "      <td>0.769164</td>\n",
       "      <td>0.730482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.772804</td>\n",
       "      <td>0.735649</td>\n",
       "      <td>0.772453</td>\n",
       "      <td>0.733538</td>\n",
       "      <td>0.772133</td>\n",
       "      <td>0.733323</td>\n",
       "      <td>0.770363</td>\n",
       "      <td>0.731641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.772691</td>\n",
       "      <td>0.736943</td>\n",
       "      <td>0.775921</td>\n",
       "      <td>0.735147</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>0.734919</td>\n",
       "      <td>0.770546</td>\n",
       "      <td>0.732713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.772303</td>\n",
       "      <td>0.732517</td>\n",
       "      <td>0.773433</td>\n",
       "      <td>0.731299</td>\n",
       "      <td>0.773219</td>\n",
       "      <td>0.731214</td>\n",
       "      <td>0.769756</td>\n",
       "      <td>0.728387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.775322</td>\n",
       "      <td>0.737181</td>\n",
       "      <td>0.777964</td>\n",
       "      <td>0.735412</td>\n",
       "      <td>0.777601</td>\n",
       "      <td>0.735258</td>\n",
       "      <td>0.772717</td>\n",
       "      <td>0.732483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.778009</td>\n",
       "      <td>0.738288</td>\n",
       "      <td>0.779599</td>\n",
       "      <td>0.735905</td>\n",
       "      <td>0.779252</td>\n",
       "      <td>0.735714</td>\n",
       "      <td>0.775504</td>\n",
       "      <td>0.733817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.777656</td>\n",
       "      <td>0.741121</td>\n",
       "      <td>0.782038</td>\n",
       "      <td>0.738797</td>\n",
       "      <td>0.781782</td>\n",
       "      <td>0.738721</td>\n",
       "      <td>0.775645</td>\n",
       "      <td>0.736140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.779829</td>\n",
       "      <td>0.741723</td>\n",
       "      <td>0.783983</td>\n",
       "      <td>0.739206</td>\n",
       "      <td>0.783506</td>\n",
       "      <td>0.739010</td>\n",
       "      <td>0.777770</td>\n",
       "      <td>0.736829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.782699</td>\n",
       "      <td>0.743006</td>\n",
       "      <td>0.785177</td>\n",
       "      <td>0.740235</td>\n",
       "      <td>0.784842</td>\n",
       "      <td>0.740136</td>\n",
       "      <td>0.780698</td>\n",
       "      <td>0.738634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>8000</td>\n",
       "      <td>0.784274</td>\n",
       "      <td>0.744707</td>\n",
       "      <td>0.786743</td>\n",
       "      <td>0.741714</td>\n",
       "      <td>0.786397</td>\n",
       "      <td>0.741641</td>\n",
       "      <td>0.782115</td>\n",
       "      <td>0.740039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>9000</td>\n",
       "      <td>0.784475</td>\n",
       "      <td>0.741555</td>\n",
       "      <td>0.784166</td>\n",
       "      <td>0.738862</td>\n",
       "      <td>0.783640</td>\n",
       "      <td>0.738529</td>\n",
       "      <td>0.782391</td>\n",
       "      <td>0.737895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.786133</td>\n",
       "      <td>0.745870</td>\n",
       "      <td>0.790783</td>\n",
       "      <td>0.743873</td>\n",
       "      <td>0.790421</td>\n",
       "      <td>0.743707</td>\n",
       "      <td>0.784282</td>\n",
       "      <td>0.741218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.787727</td>\n",
       "      <td>0.744989</td>\n",
       "      <td>0.788661</td>\n",
       "      <td>0.742498</td>\n",
       "      <td>0.788276</td>\n",
       "      <td>0.742295</td>\n",
       "      <td>0.785881</td>\n",
       "      <td>0.741104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.784762</td>\n",
       "      <td>0.742947</td>\n",
       "      <td>0.789025</td>\n",
       "      <td>0.740939</td>\n",
       "      <td>0.788611</td>\n",
       "      <td>0.740732</td>\n",
       "      <td>0.782649</td>\n",
       "      <td>0.738281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.784561</td>\n",
       "      <td>0.741339</td>\n",
       "      <td>0.787504</td>\n",
       "      <td>0.739586</td>\n",
       "      <td>0.787338</td>\n",
       "      <td>0.739606</td>\n",
       "      <td>0.781763</td>\n",
       "      <td>0.736401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.784888</td>\n",
       "      <td>0.743257</td>\n",
       "      <td>0.789322</td>\n",
       "      <td>0.740997</td>\n",
       "      <td>0.788866</td>\n",
       "      <td>0.740824</td>\n",
       "      <td>0.783122</td>\n",
       "      <td>0.738891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.785525</td>\n",
       "      <td>0.743107</td>\n",
       "      <td>0.789527</td>\n",
       "      <td>0.740765</td>\n",
       "      <td>0.789221</td>\n",
       "      <td>0.740768</td>\n",
       "      <td>0.783457</td>\n",
       "      <td>0.738538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.786237</td>\n",
       "      <td>0.744466</td>\n",
       "      <td>0.791212</td>\n",
       "      <td>0.742230</td>\n",
       "      <td>0.790868</td>\n",
       "      <td>0.742206</td>\n",
       "      <td>0.784248</td>\n",
       "      <td>0.739546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.785556</td>\n",
       "      <td>0.743784</td>\n",
       "      <td>0.791293</td>\n",
       "      <td>0.741823</td>\n",
       "      <td>0.790829</td>\n",
       "      <td>0.741639</td>\n",
       "      <td>0.783539</td>\n",
       "      <td>0.739009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.787591</td>\n",
       "      <td>0.744358</td>\n",
       "      <td>0.791955</td>\n",
       "      <td>0.742205</td>\n",
       "      <td>0.791680</td>\n",
       "      <td>0.742196</td>\n",
       "      <td>0.785513</td>\n",
       "      <td>0.739775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>8000</td>\n",
       "      <td>0.788182</td>\n",
       "      <td>0.744136</td>\n",
       "      <td>0.792550</td>\n",
       "      <td>0.742192</td>\n",
       "      <td>0.792331</td>\n",
       "      <td>0.742276</td>\n",
       "      <td>0.785787</td>\n",
       "      <td>0.739311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>9000</td>\n",
       "      <td>0.788718</td>\n",
       "      <td>0.745299</td>\n",
       "      <td>0.793465</td>\n",
       "      <td>0.743135</td>\n",
       "      <td>0.793109</td>\n",
       "      <td>0.743055</td>\n",
       "      <td>0.786520</td>\n",
       "      <td>0.740571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.790324</td>\n",
       "      <td>0.745950</td>\n",
       "      <td>0.793678</td>\n",
       "      <td>0.743267</td>\n",
       "      <td>0.793411</td>\n",
       "      <td>0.743305</td>\n",
       "      <td>0.788094</td>\n",
       "      <td>0.741284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.790136</td>\n",
       "      <td>0.745577</td>\n",
       "      <td>0.793582</td>\n",
       "      <td>0.743069</td>\n",
       "      <td>0.793339</td>\n",
       "      <td>0.743119</td>\n",
       "      <td>0.787587</td>\n",
       "      <td>0.740733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.790327</td>\n",
       "      <td>0.744636</td>\n",
       "      <td>0.792993</td>\n",
       "      <td>0.742012</td>\n",
       "      <td>0.792737</td>\n",
       "      <td>0.742020</td>\n",
       "      <td>0.787753</td>\n",
       "      <td>0.739992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.788880</td>\n",
       "      <td>0.744988</td>\n",
       "      <td>0.794125</td>\n",
       "      <td>0.742824</td>\n",
       "      <td>0.793907</td>\n",
       "      <td>0.742957</td>\n",
       "      <td>0.786535</td>\n",
       "      <td>0.739993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.788778</td>\n",
       "      <td>0.744506</td>\n",
       "      <td>0.794279</td>\n",
       "      <td>0.742456</td>\n",
       "      <td>0.793956</td>\n",
       "      <td>0.742451</td>\n",
       "      <td>0.786306</td>\n",
       "      <td>0.739491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.744675</td>\n",
       "      <td>0.794138</td>\n",
       "      <td>0.742266</td>\n",
       "      <td>0.793854</td>\n",
       "      <td>0.742283</td>\n",
       "      <td>0.787789</td>\n",
       "      <td>0.739918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.790184</td>\n",
       "      <td>0.744669</td>\n",
       "      <td>0.794069</td>\n",
       "      <td>0.742288</td>\n",
       "      <td>0.793766</td>\n",
       "      <td>0.742320</td>\n",
       "      <td>0.787550</td>\n",
       "      <td>0.739678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.789604</td>\n",
       "      <td>0.744989</td>\n",
       "      <td>0.794845</td>\n",
       "      <td>0.742748</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.742793</td>\n",
       "      <td>0.786910</td>\n",
       "      <td>0.739697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.790219</td>\n",
       "      <td>0.745070</td>\n",
       "      <td>0.794921</td>\n",
       "      <td>0.742709</td>\n",
       "      <td>0.794587</td>\n",
       "      <td>0.742719</td>\n",
       "      <td>0.787642</td>\n",
       "      <td>0.740010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4</td>\n",
       "      <td>8000</td>\n",
       "      <td>0.790501</td>\n",
       "      <td>0.744826</td>\n",
       "      <td>0.794558</td>\n",
       "      <td>0.742350</td>\n",
       "      <td>0.794203</td>\n",
       "      <td>0.742349</td>\n",
       "      <td>0.787834</td>\n",
       "      <td>0.739858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4</td>\n",
       "      <td>9000</td>\n",
       "      <td>0.790475</td>\n",
       "      <td>0.744919</td>\n",
       "      <td>0.794755</td>\n",
       "      <td>0.742490</td>\n",
       "      <td>0.794375</td>\n",
       "      <td>0.742465</td>\n",
       "      <td>0.787842</td>\n",
       "      <td>0.739924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.790394</td>\n",
       "      <td>0.745216</td>\n",
       "      <td>0.795033</td>\n",
       "      <td>0.742782</td>\n",
       "      <td>0.794664</td>\n",
       "      <td>0.742757</td>\n",
       "      <td>0.787817</td>\n",
       "      <td>0.740152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.790404</td>\n",
       "      <td>0.745216</td>\n",
       "      <td>0.795036</td>\n",
       "      <td>0.742784</td>\n",
       "      <td>0.794667</td>\n",
       "      <td>0.742758</td>\n",
       "      <td>0.787826</td>\n",
       "      <td>0.740154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  steps  cosine_pearson  cosine_spearman  euclidean_pearson  \\\n",
       "0       0   1000        0.468945         0.476004           0.403510   \n",
       "1       0   2000        0.592885         0.607386           0.561215   \n",
       "2       0   3000        0.639567         0.645881           0.618728   \n",
       "3       0   4000        0.662008         0.663198           0.655926   \n",
       "4       0   5000        0.680992         0.676275           0.677499   \n",
       "5       0   6000        0.695814         0.688548           0.691513   \n",
       "6       0   7000        0.704021         0.694879           0.702576   \n",
       "7       0   8000        0.712582         0.700064           0.708250   \n",
       "8       0   9000        0.716562         0.702152           0.723412   \n",
       "9       0  10000        0.724499         0.702371           0.720826   \n",
       "10      0     -1        0.728147         0.708525           0.727829   \n",
       "11      1   1000        0.733975         0.711244           0.737156   \n",
       "12      1   2000        0.737847         0.719376           0.745958   \n",
       "13      1   3000        0.746105         0.720037           0.748386   \n",
       "14      1   4000        0.749436         0.719457           0.749008   \n",
       "15      1   5000        0.755005         0.726465           0.757816   \n",
       "16      1   6000        0.757062         0.725729           0.758183   \n",
       "17      1   7000        0.762997         0.731823           0.764398   \n",
       "18      1   8000        0.767116         0.734692           0.767075   \n",
       "19      1   9000        0.767641         0.734926           0.771441   \n",
       "20      1  10000        0.771579         0.734489           0.772224   \n",
       "21      1     -1        0.772804         0.735649           0.772453   \n",
       "22      2   1000        0.772691         0.736943           0.775921   \n",
       "23      2   2000        0.772303         0.732517           0.773433   \n",
       "24      2   3000        0.775322         0.737181           0.777964   \n",
       "25      2   4000        0.778009         0.738288           0.779599   \n",
       "26      2   5000        0.777656         0.741121           0.782038   \n",
       "27      2   6000        0.779829         0.741723           0.783983   \n",
       "28      2   7000        0.782699         0.743006           0.785177   \n",
       "29      2   8000        0.784274         0.744707           0.786743   \n",
       "30      2   9000        0.784475         0.741555           0.784166   \n",
       "31      2  10000        0.786133         0.745870           0.790783   \n",
       "32      2     -1        0.787727         0.744989           0.788661   \n",
       "33      3   1000        0.784762         0.742947           0.789025   \n",
       "34      3   2000        0.784561         0.741339           0.787504   \n",
       "35      3   3000        0.784888         0.743257           0.789322   \n",
       "36      3   4000        0.785525         0.743107           0.789527   \n",
       "37      3   5000        0.786237         0.744466           0.791212   \n",
       "38      3   6000        0.785556         0.743784           0.791293   \n",
       "39      3   7000        0.787591         0.744358           0.791955   \n",
       "40      3   8000        0.788182         0.744136           0.792550   \n",
       "41      3   9000        0.788718         0.745299           0.793465   \n",
       "42      3  10000        0.790324         0.745950           0.793678   \n",
       "43      3     -1        0.790136         0.745577           0.793582   \n",
       "44      4   1000        0.790327         0.744636           0.792993   \n",
       "45      4   2000        0.788880         0.744988           0.794125   \n",
       "46      4   3000        0.788778         0.744506           0.794279   \n",
       "47      4   4000        0.790323         0.744675           0.794138   \n",
       "48      4   5000        0.790184         0.744669           0.794069   \n",
       "49      4   6000        0.789604         0.744989           0.794845   \n",
       "50      4   7000        0.790219         0.745070           0.794921   \n",
       "51      4   8000        0.790501         0.744826           0.794558   \n",
       "52      4   9000        0.790475         0.744919           0.794755   \n",
       "53      4  10000        0.790394         0.745216           0.795033   \n",
       "54      4     -1        0.790404         0.745216           0.795036   \n",
       "\n",
       "    euclidean_spearman  manhattan_pearson  manhattan_spearman  dot_pearson  \\\n",
       "0             0.424756           0.403969            0.425364     0.487691   \n",
       "1             0.572892           0.562487            0.574276     0.596222   \n",
       "2             0.620769           0.619136            0.621277     0.638827   \n",
       "3             0.650820           0.656257            0.651380     0.658362   \n",
       "4             0.667278           0.677414            0.667357     0.675948   \n",
       "5             0.679803           0.691554            0.680018     0.690855   \n",
       "6             0.687894           0.702605            0.688137     0.699132   \n",
       "7             0.691851           0.708132            0.691822     0.707009   \n",
       "8             0.700258           0.723382            0.700354     0.712686   \n",
       "9             0.698476           0.720519            0.698312     0.719167   \n",
       "10            0.704231           0.727771            0.704261     0.724349   \n",
       "11            0.709016           0.737057            0.709074     0.730813   \n",
       "12            0.716687           0.745550            0.716507     0.736410   \n",
       "13            0.717082           0.748117            0.716998     0.744367   \n",
       "14            0.716512           0.748800            0.716348     0.745617   \n",
       "15            0.724752           0.757464            0.724589     0.752393   \n",
       "16            0.723777           0.757483            0.723227     0.754772   \n",
       "17            0.728876           0.764066            0.728718     0.761528   \n",
       "18            0.731277           0.766908            0.731326     0.765289   \n",
       "19            0.733516           0.771302            0.733475     0.765339   \n",
       "20            0.732795           0.771878            0.732576     0.769164   \n",
       "21            0.733538           0.772133            0.733323     0.770363   \n",
       "22            0.735147           0.775444            0.734919     0.770546   \n",
       "23            0.731299           0.773219            0.731214     0.769756   \n",
       "24            0.735412           0.777601            0.735258     0.772717   \n",
       "25            0.735905           0.779252            0.735714     0.775504   \n",
       "26            0.738797           0.781782            0.738721     0.775645   \n",
       "27            0.739206           0.783506            0.739010     0.777770   \n",
       "28            0.740235           0.784842            0.740136     0.780698   \n",
       "29            0.741714           0.786397            0.741641     0.782115   \n",
       "30            0.738862           0.783640            0.738529     0.782391   \n",
       "31            0.743873           0.790421            0.743707     0.784282   \n",
       "32            0.742498           0.788276            0.742295     0.785881   \n",
       "33            0.740939           0.788611            0.740732     0.782649   \n",
       "34            0.739586           0.787338            0.739606     0.781763   \n",
       "35            0.740997           0.788866            0.740824     0.783122   \n",
       "36            0.740765           0.789221            0.740768     0.783457   \n",
       "37            0.742230           0.790868            0.742206     0.784248   \n",
       "38            0.741823           0.790829            0.741639     0.783539   \n",
       "39            0.742205           0.791680            0.742196     0.785513   \n",
       "40            0.742192           0.792331            0.742276     0.785787   \n",
       "41            0.743135           0.793109            0.743055     0.786520   \n",
       "42            0.743267           0.793411            0.743305     0.788094   \n",
       "43            0.743069           0.793339            0.743119     0.787587   \n",
       "44            0.742012           0.792737            0.742020     0.787753   \n",
       "45            0.742824           0.793907            0.742957     0.786535   \n",
       "46            0.742456           0.793956            0.742451     0.786306   \n",
       "47            0.742266           0.793854            0.742283     0.787789   \n",
       "48            0.742288           0.793766            0.742320     0.787550   \n",
       "49            0.742748           0.794500            0.742793     0.786910   \n",
       "50            0.742709           0.794587            0.742719     0.787642   \n",
       "51            0.742350           0.794203            0.742349     0.787834   \n",
       "52            0.742490           0.794375            0.742465     0.787842   \n",
       "53            0.742782           0.794664            0.742757     0.787817   \n",
       "54            0.742784           0.794667            0.742758     0.787826   \n",
       "\n",
       "    dot_spearman  \n",
       "0       0.492376  \n",
       "1       0.598063  \n",
       "2       0.634351  \n",
       "3       0.650780  \n",
       "4       0.664827  \n",
       "5       0.678142  \n",
       "6       0.684231  \n",
       "7       0.689909  \n",
       "8       0.693195  \n",
       "9       0.696440  \n",
       "10      0.701995  \n",
       "11      0.705273  \n",
       "12      0.712277  \n",
       "13      0.715522  \n",
       "14      0.714427  \n",
       "15      0.721438  \n",
       "16      0.721516  \n",
       "17      0.727469  \n",
       "18      0.730022  \n",
       "19      0.729942  \n",
       "20      0.730482  \n",
       "21      0.731641  \n",
       "22      0.732713  \n",
       "23      0.728387  \n",
       "24      0.732483  \n",
       "25      0.733817  \n",
       "26      0.736140  \n",
       "27      0.736829  \n",
       "28      0.738634  \n",
       "29      0.740039  \n",
       "30      0.737895  \n",
       "31      0.741218  \n",
       "32      0.741104  \n",
       "33      0.738281  \n",
       "34      0.736401  \n",
       "35      0.738891  \n",
       "36      0.738538  \n",
       "37      0.739546  \n",
       "38      0.739009  \n",
       "39      0.739775  \n",
       "40      0.739311  \n",
       "41      0.740571  \n",
       "42      0.741284  \n",
       "43      0.740733  \n",
       "44      0.739992  \n",
       "45      0.739993  \n",
       "46      0.739491  \n",
       "47      0.739918  \n",
       "48      0.739678  \n",
       "49      0.739697  \n",
       "50      0.740010  \n",
       "51      0.739858  \n",
       "52      0.739924  \n",
       "53      0.740152  \n",
       "54      0.740154  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_model_train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beec656b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>steps</th>\n",
       "      <th>cosine_pearson</th>\n",
       "      <th>cosine_spearman</th>\n",
       "      <th>euclidean_pearson</th>\n",
       "      <th>euclidean_spearman</th>\n",
       "      <th>manhattan_pearson</th>\n",
       "      <th>manhattan_spearman</th>\n",
       "      <th>dot_pearson</th>\n",
       "      <th>dot_spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.426726</td>\n",
       "      <td>0.430551</td>\n",
       "      <td>0.378483</td>\n",
       "      <td>0.409842</td>\n",
       "      <td>0.378912</td>\n",
       "      <td>0.410433</td>\n",
       "      <td>0.414030</td>\n",
       "      <td>0.423347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.531125</td>\n",
       "      <td>0.542858</td>\n",
       "      <td>0.476350</td>\n",
       "      <td>0.515459</td>\n",
       "      <td>0.476494</td>\n",
       "      <td>0.515582</td>\n",
       "      <td>0.531259</td>\n",
       "      <td>0.541011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.575994</td>\n",
       "      <td>0.583144</td>\n",
       "      <td>0.527919</td>\n",
       "      <td>0.561577</td>\n",
       "      <td>0.527666</td>\n",
       "      <td>0.561342</td>\n",
       "      <td>0.579291</td>\n",
       "      <td>0.582554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.601131</td>\n",
       "      <td>0.603386</td>\n",
       "      <td>0.565349</td>\n",
       "      <td>0.590296</td>\n",
       "      <td>0.565122</td>\n",
       "      <td>0.590056</td>\n",
       "      <td>0.606113</td>\n",
       "      <td>0.603552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.614978</td>\n",
       "      <td>0.611843</td>\n",
       "      <td>0.562131</td>\n",
       "      <td>0.590615</td>\n",
       "      <td>0.561770</td>\n",
       "      <td>0.590222</td>\n",
       "      <td>0.616959</td>\n",
       "      <td>0.614163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.727587</td>\n",
       "      <td>0.688705</td>\n",
       "      <td>0.718137</td>\n",
       "      <td>0.685859</td>\n",
       "      <td>0.717924</td>\n",
       "      <td>0.685596</td>\n",
       "      <td>0.734381</td>\n",
       "      <td>0.700907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4</td>\n",
       "      <td>8000</td>\n",
       "      <td>0.728326</td>\n",
       "      <td>0.688412</td>\n",
       "      <td>0.717214</td>\n",
       "      <td>0.685106</td>\n",
       "      <td>0.716983</td>\n",
       "      <td>0.684822</td>\n",
       "      <td>0.735092</td>\n",
       "      <td>0.700720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4</td>\n",
       "      <td>9000</td>\n",
       "      <td>0.728463</td>\n",
       "      <td>0.688878</td>\n",
       "      <td>0.718223</td>\n",
       "      <td>0.685705</td>\n",
       "      <td>0.717992</td>\n",
       "      <td>0.685410</td>\n",
       "      <td>0.735255</td>\n",
       "      <td>0.701168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.728188</td>\n",
       "      <td>0.689130</td>\n",
       "      <td>0.718569</td>\n",
       "      <td>0.686004</td>\n",
       "      <td>0.718342</td>\n",
       "      <td>0.685722</td>\n",
       "      <td>0.735058</td>\n",
       "      <td>0.701414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.728183</td>\n",
       "      <td>0.689132</td>\n",
       "      <td>0.718574</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.718347</td>\n",
       "      <td>0.685717</td>\n",
       "      <td>0.735054</td>\n",
       "      <td>0.701415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  steps  cosine_pearson  cosine_spearman  euclidean_pearson  \\\n",
       "0       0   1000        0.426726         0.430551           0.378483   \n",
       "1       0   2000        0.531125         0.542858           0.476350   \n",
       "2       0   3000        0.575994         0.583144           0.527919   \n",
       "3       0   4000        0.601131         0.603386           0.565349   \n",
       "4       0   5000        0.614978         0.611843           0.562131   \n",
       "..    ...    ...             ...              ...                ...   \n",
       "62      4   7000        0.727587         0.688705           0.718137   \n",
       "63      4   8000        0.728326         0.688412           0.717214   \n",
       "64      4   9000        0.728463         0.688878           0.718223   \n",
       "65      4  10000        0.728188         0.689130           0.718569   \n",
       "66      4     -1        0.728183         0.689132           0.718574   \n",
       "\n",
       "    euclidean_spearman  manhattan_pearson  manhattan_spearman  dot_pearson  \\\n",
       "0             0.409842           0.378912            0.410433     0.414030   \n",
       "1             0.515459           0.476494            0.515582     0.531259   \n",
       "2             0.561577           0.527666            0.561342     0.579291   \n",
       "3             0.590296           0.565122            0.590056     0.606113   \n",
       "4             0.590615           0.561770            0.590222     0.616959   \n",
       "..                 ...                ...                 ...          ...   \n",
       "62            0.685859           0.717924            0.685596     0.734381   \n",
       "63            0.685106           0.716983            0.684822     0.735092   \n",
       "64            0.685705           0.717992            0.685410     0.735255   \n",
       "65            0.686004           0.718342            0.685722     0.735058   \n",
       "66            0.686000           0.718347            0.685717     0.735054   \n",
       "\n",
       "    dot_spearman  \n",
       "0       0.423347  \n",
       "1       0.541011  \n",
       "2       0.582554  \n",
       "3       0.603552  \n",
       "4       0.614163  \n",
       "..           ...  \n",
       "62      0.700907  \n",
       "63      0.700720  \n",
       "64      0.701168  \n",
       "65      0.701414  \n",
       "66      0.701415  \n",
       "\n",
       "[67 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_model_train_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b207ba5",
   "metadata": {},
   "source": [
    "با توجه به نتایج بالا، می توان به گزاره های زیر رسید:\n",
    " - با توجه یه اینکه مقدار معیار ها در ابتدار پایین بوده و در طول جریان یادگیری رو به رشد بوده اند، می توان گفت که یادگیری موفق بوده است.\n",
    " - مقدار معیار های ارزیابی برای مدل آموزش دیده روی داده های خام بیشتر از داده های پیش پردازش شده است. این مورد ثابت می کند که حذف کردن ایست واژه ها و تغییراتی از این دست، نه تنها اثر مثبت نداشته، بلکه مخرب نیز بوده است.\n",
    " - دقت مدل از گام ۴۴ام تغییرات مثبت چندانی نداشته است که نشان می دهد برای بهبود مدل باید با مقادیر پایینتری از learning rate و یا تغییراتی در پارامتر های دیگر آموزش را ادامه داد. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc59682",
   "metadata": {},
   "source": [
    "برای مقایسه با مدل آموزش ندیده، همانند بالا، بهترین حد مرجعی که مدل می تواند جملات مشابه را از یکدیگر تشخیص دهد پیدا می کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb9b625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c185d7a88b4846b49421311eaacf12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16793 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404287it [00:55, 7301.53it/s]\n",
      "100%|██████████| 70/70 [00:19<00:00,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5800000000000003, 95.52),\n",
      " (0.5900000000000003, 95.52),\n",
      " (0.5700000000000003, 95.51),\n",
      " (0.6000000000000003, 95.51),\n",
      " (0.6100000000000003, 95.51),\n",
      " (0.5600000000000003, 95.5),\n",
      " (0.6200000000000003, 95.49),\n",
      " (0.5500000000000003, 95.47),\n",
      " (0.6300000000000003, 95.47),\n",
      " (0.5400000000000003, 95.44)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_model = SentenceTransformer('raw_model_training_results')\n",
    "raw_questions = list(set(data['question1'].to_list() + data['question2'].to_list()))\n",
    "print(len(raw_questions))\n",
    "raw_questions_embeddings = raw_model.encode(raw_questions,\n",
    "                                            show_progress_bar=True,\n",
    "                                            device='cuda')\n",
    "raw_questions_embeddings_dict = {key: value for key, value in zip(raw_questions, raw_questions_embeddings)}\n",
    "\n",
    "print(len(raw_questions_embeddings_dict))\n",
    "\n",
    "question1_embeddings_raw_model = [raw_questions_embeddings_dict.get(text) for text in data['question1']]\n",
    "question2_embeddings_raw_model = [raw_questions_embeddings_dict.get(text) for text in data['question2']]\n",
    "\n",
    "raw_similarities = []\n",
    "for item in tqdm.tqdm(zip(question1_embeddings_raw_model, question2_embeddings_raw_model)):\n",
    "    raw_similarities.append(float(util.cos_sim(item[0], item[1])))\n",
    "\n",
    "raw_accuracies = get_accuracies(raw_similarities, data['is_duplicate'], thresholds)\n",
    "\n",
    "pprint(raw_accuracies[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f6a15",
   "metadata": {},
   "source": [
    "همانطور که دیده می شود، مدل آموزش دیده می تواند با دقت بسیار بیشتری جملات مشابه را تشخیص دهد. همچنین، این تشخیص شباهت در حد مرجع های پایینتری صورت می گیرد. این مورد می تواند به این معنی باشد که ویژگی هایی که در بردارهای خروجی برای جملات وجود دارند، بیشتر برای تشخیص شباهت آموزش دیده اند. چون وقتی حد مرجع بالا باشد (مثلا ۰.۹) یعنی جملاتی که مثلا ۸۹ درصد به هم شبیه اند، در واقع شبیه هم در نظر گرفته نمی شوند. این خود نشان دهنده این موضوع است که بردار های تولید شده پیش از آموزش مدل، بیشتر به هم شبیه بوده اند اما پس از آموزش مدل، از این شباهت ها کاسته شده و اختلاف بین جملات، بهتر یاد گرفته شده است. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f362d",
   "metadata": {},
   "source": [
    "# بخش دوم و سوم"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e41b60",
   "metadata": {},
   "source": [
    "برای تسک تشخیص شباهت، یکی از بهترین و موثرترین تکنیک ها همین استفاده از بردار های جملات است. این روش نیز خود وابسته به درون نگاری های ارائه شده توسط مدل های پیشرو مثل BERT است. یا این حال، می توان همانند سایر کارهای دسته بندی، مدلی برای دسته بندی زوج جملات پیاده سازی کرد. همانطور که پیش تر اشاره شد، گونه ای از مدل ها که در همین راستا پیاده سازی شده اند، مدل های Siamese هستند. در اینجا نیز ما سعی می کنیم یک مدل Siamese بر پایه LSTM پیاده سازی کنیم و عملکرد آن را با مدل BERT مقایسه کنیم.\n",
    "\n",
    "این مدل متشکل از یک لایه Embedding، یک (یا چند) لایه LSTM و در نهایت، یک یا چند لایه Linear برای دسته بندی داده هاست. در ساده ترین حالت، لایه Embedding، یک لایه LSTM و یک لایه Linear به کار گرفته می شوند. در ادامه می توان ساختار شبکه را پیچیده تر کرد و از Embedding های از پیش آموزش دیده و با ابعاد بزرگتر بهره گرفت.\n",
    "\n",
    "روند کار به این صورت است که دو عبارت ورودی به شکل مستقل به شبکه LSTM داده می شوند (منظور از مستقل این است که مقادیر hidden و cell  برای هر جمله جداگانه ذخیره می شود). سپس بردار های encode شده به یکدیگر چسبانده (concat) می شوند و به عنوان ویژگی های استخراج شده به لایه Linear یا دسته بند داده می شوند (در معماری های پیچیده تر، می توان ترکیبات خطی و غیر خطی مختلفی از بردار های encode شده حاصل از LSTM را به لایه دسته بند داد).سپس لایه دسته بند بردار های ورودی را به بردار های دوتایی نگاشت می کند (دوتایی از آن جهت که دو برچسب مشابه و غیر مشابه داریم)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd58669",
   "metadata": {},
   "source": [
    "در ابتدا لازم است که نگاشتی از همه ی کلمات دیتاست به اعداد داشته باشیم (لایه Embedding باید کلمات را به شکل اعداد ببیند تا بتواند درون نگاری آن ها را یاد بگیرد)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fce6ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for sentence in data['question1'].to_list() + data['question2'].to_list() for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b39d32f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8944556"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0e35062",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dde999dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232531"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b227ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word: idx for idx, word in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9795efb8",
   "metadata": {},
   "source": [
    "سپس توکن های ویژه شروع و پایان را نیز به دامنه کلمات اضافه می کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23400d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx['<SOS>'] = len(word_to_idx)\n",
    "word_to_idx['<EOS>'] = len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fd6d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {key: value for value, key in word_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ca132",
   "metadata": {},
   "source": [
    "با استفاده از تابع `sequences_to_tensor` رشته های جملات را به بردارهای قابل قبول برای شبکه پیاده سازی شده تبدیل می کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b322c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_tensor(sequences, device):\n",
    "    results = []\n",
    "    for sequence in sequences:\n",
    "        words = sequence.split()\n",
    "        words = ['<SOS>'] + words + ['<EOS>']\n",
    "        indices = [word_to_idx.get(word) for word in words]\n",
    "        results.append(torch.tensor(indices), device=device)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bba71",
   "metadata": {},
   "source": [
    "مدل SiameseLSTM به همراه LSTMEncoder به شکل زیر پیاده سازی می شوند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc4fa55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_size,\n",
    "                                  num_layers=self.num_layers, batch_first=True)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = torch.autograd.Variable(torch.randn(self.num_layers, self.hidden_size)).cuda()\n",
    "        cell = torch.autograd.Variable(torch.randn(self.num_layers, self.hidden_size)).cuda()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = self.embedding(input)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        \n",
    "        return output, hidden, cell\n",
    "\n",
    "\n",
    "class SiameseLSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = LSTMEncoder(vocab_size, embedding_dim, hidden_size, num_layers)\n",
    "\n",
    "        self.classifier = torch.nn.Linear(2 * self.encoder.hidden_size, 2, )\n",
    "        self.softmax = torch.nn.functional.softmax\n",
    "        \n",
    "    def forward(self, s1, s2):\n",
    "\n",
    "        h1, c1 = self.encoder.init_hidden()\n",
    "        h2, c2 = self.encoder.init_hidden()\n",
    "\n",
    "        for i in range(len(s1)):\n",
    "\n",
    "            v1, h1, c1 = self.encoder(s1[i], h1, c1)\n",
    "\n",
    "        for j in range(len(s2)):\n",
    "            v2, h2, c2 = self.encoder(s2[j], h2, c2)\n",
    "        \n",
    "        # Create features for classification. This can be extended for more complex features.\n",
    "        features = torch.cat((h1, h2), 1)\n",
    "\n",
    "        output = self.classifier(features)\n",
    "        output = self.softmax(output, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b27b512",
   "metadata": {},
   "source": [
    "برای آماده کردن داده برای آموزش این شبکه، کلاس دیتاست لازم به شکل زیر پیاده سازی می شود."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c7fe885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset():\n",
    "    def __init__(self, question1_list, question2_list, labels):\n",
    "        self.first_questions = question1_list\n",
    "        self.second_questions = question2_list\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.first_questions[index],\n",
    "                self.second_questions[index],\n",
    "                self.labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97adbed6",
   "metadata": {},
   "source": [
    "داده های لازم برای آموزش و نیز تست مدل به شکل زیر آماده می شوند. تقسیم داده ها به شکل ۸۰ درصد آموزش، ۲۰ درصد تست است."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a36c43d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_split = int(len(data) * 0.8)\n",
    "\n",
    "train_ds = CustomDataset(data['question1'].to_list()[:train_validation_split],\n",
    "                         data['question2'].to_list()[:train_validation_split],\n",
    "                         data['is_duplicate'].to_list()[:train_validation_split])\n",
    "\n",
    "val_ds = CustomDataset(data['question1'].to_list()[train_validation_split:],\n",
    "                       data['question2'].to_list()[train_validation_split:],\n",
    "                       data['is_duplicate'].to_list()[train_validation_split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1f398",
   "metadata": {},
   "source": [
    "پارامتر های دخیل در آزمایش به شکل زیر تعریف  می شوند. برای به دست آمدن بهترین نتیجه، لازم است تا با تغییر پارامتر های زیر و آموشش مجدد، بهترین ترکیب از پارامتر ها را یافت."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26afc3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "embedding_dim = 100\n",
    "hidden_size = 10\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7669ae",
   "metadata": {},
   "source": [
    "مدل و نیز Optimizer و تابع Loss، به شکل زیر تعریف می شوند. با توجه به اینکه مدل دسته بندی زوج سوالات داده شده را تولید می کند، می توان از تابع `CrossEntropy` استفاده کرد که برای کارهای دسته بندی مناسب است."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0a47d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_lstm = SiameseLSTM(vocab_size, embedding_dim, hidden_size, num_layers)\n",
    "\n",
    "loss_weights = torch.autograd.Variable(torch.FloatTensor([1, 3]))\n",
    "if torch.cuda.is_available():\n",
    "    loss_weights = loss_weights.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss(loss_weights)\n",
    "optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, siamese_lstm.parameters()), lr=learning_rate)\n",
    "\n",
    "\n",
    "criterion = criterion.cuda()\n",
    "siamese_lstm = siamese_lstm.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf92ae",
   "metadata": {},
   "source": [
    "یک نمونه خروجی تولید می شود تا از صحت کارکرد مدل اطمینان حاصل شود."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61197bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4096, 0.5904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_lstm(sequences_to_tensor(['first string'], 'cuda'), sequences_to_tensor(['second good string'], 'cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495033e2",
   "metadata": {},
   "source": [
    "در ادامه کد آموزش مدل پیاده سازی شده است. با توجه اینکه این مدل یک لایه LSTM و یک لایه Embedding دارد که نیاز به آموزش دارند، پروسه آموزش این مدل بسیار زمانبر خواهد بود. با توجه به محدودیت زمان، امکان آموزش این مدل فراهم نشد."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe9280",
   "metadata": {},
   "source": [
    "علیرغم اینکه امکان آموزش مدل فراهم نشد، می توان برای بخش سوم پروژه و بهبود معماری فعلی راهکارهایی ارائه داد. راهکار های زیر می توانند در بهبود عملکرد مدل در تشخیص شباهت موثر باشند، هرچمد به نظر نمی رسد این مدل بتواند بهتر از مدل SentenceTransformer که بر پایه مدل BERT است، عمل کند:\n",
    "\n",
    "- افزایش لایه های LSTM (افزایش `num_layers`)\n",
    "- افزایش ابعاد hidden state (افزایش `hiddden_size`)\n",
    "- افزایش ابعاد درون نگاری در لایه Embedding (افزایش `embedding_dim`)\n",
    "- استفاده از لایه های پیچیده تر برای classifier (به جای یک لایه Linear ساده)\n",
    "- اضافه کردن ویژگی های بیشتر به بردار ویژگی ها (در حال حاضر فقط دو بردار به هم چسبانده می شوند، می توان تفاضل، شباهت کسینوسی، حاصل ضرب و ... دو بردار encode شده را در بردار ویژگی گنجاند)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_record = []\n",
    "valid_loss_record = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_loss = []\n",
    "    train_dataloader = DataLoader(dataset=train_ds, shuffle=True, num_workers=2)\n",
    "\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "\n",
    "        s1, s2, label = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = siamese_lstm(sequences_to_tensor(s1, 'cuda'), sequences_to_tensor(s2, 'cuda'))\n",
    "\n",
    "        label = torch.autograd.Variable(label)\n",
    "        if torch.cuda.is_available():\n",
    "            label = label.cuda()\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fa7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7328b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "direction": "rtl",
  "kernelspec": {
   "display_name": "nlp-proj-venv",
   "language": "python",
   "name": "nlp-proj-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
